{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime,timedelta\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据log_index，增量读取log内容\n",
    "def load_log(file_name, log_index):\n",
    "    log_value_list = list()\n",
    "    date_list = list()\n",
    "    status_list = list()\n",
    "    try:\n",
    "        with open(file_name,'rb') as f:\n",
    "            for index,content in enumerate(f):\n",
    "                if index == log_index:\n",
    "                    log_data = json.loads(content.decode('utf8'))\n",
    "                    log_data['@timestamp'] = str(datetime.strptime(log_data['@timestamp'][0:19],'%Y-%m-%dT%H:%M:%S'))\n",
    "                    log_value_list.append(log_data)\n",
    "                    date_list.append(log_data['@timestamp'])\n",
    "                    log_index += 1\n",
    "                    if log_data['status'] not in status_list:\n",
    "                        status_list.append(log_data['status'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    #将文本内容转化为pandas的dataframe\n",
    "    #temp_log = pd.DataFrame(log_value_list)\n",
    "    print(status_list)\n",
    "    return  log_value_list,log_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_urls_in_op = 20 #关键参数，每x个url作为一组进行分配\n",
    "n_ops_in_behavior = 10 #关键参数，每一个数据的行为包含多少个op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200', '302', '304', '403', '499', '404', '503', '408', '400', '504']\n"
     ]
    }
   ],
   "source": [
    "time_step = 120 #每x秒读取一次log文件\n",
    "time_windows = 120 #秒,时间窗口\n",
    "ip_dict = dict() #ip与对应操作在矩阵的下标\n",
    "ip_reserve_list = list() #操作矩阵下标对应的ip\n",
    "all_matrix = np.empty([0, 6],dtype=int)  #操作矩阵\n",
    "all_log = pd.DataFrame() #所有的日志数据\n",
    "temp_log_data = pd.DataFrame() #某时间范围的日志数据\n",
    "all_url_list = list()\n",
    "time_index = '2017-07-10 06:25:08' #记录当前的时间\n",
    "log_index = 0 #记录读到第几行了\n",
    "ip_behavior_dict = dict() #记录每个ip每分钟请求的页面url\n",
    "\n",
    "increment_log_data, log_index = load_log('0713_access_sdsl_shunnengnet_com_json.log', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#截取url，保留其访问传参方式（POST/GET/PUT/DELETE）,保留url的头与尾\n",
    "def deal_url(url):\n",
    "    url_str = url.replace(' HTTP/1.1','')\n",
    "    if url_str.find(' ') > -1:\n",
    "        url_type = url_str[0 : url_str.find(' ')+1]\n",
    "    if url_str.find('/') > -1 and url_str.find('?') > -1:\n",
    "        url_str = url_type + url_str[url_str.find('/')+1 : url_str.find('?')]\n",
    "    if url_str.find('/') > -1 and url_str.rfind('/') > -1:\n",
    "        url_str = url_str[0 : url_str.find('/')] + '/.../' + url_str[url_str.rfind('/')+1 : ]\n",
    "    return url_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据日志内容，创建一个信息的op_detail\n",
    "def create_op_detail(row_log):\n",
    "    op_detail = dict()\n",
    "    op_detail['start_time'] = row_log['@timestamp']\n",
    "    op_detail['end_time'] = row_log['@timestamp']\n",
    "    op_detail['urls'] = list()\n",
    "    op_detail['urls'].append(deal_url(row_log['request']))\n",
    "    return op_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#传入开始结束时间的字符串(yyyy-MM-dd HH:mm:ss)，计算时间差并返回时差(秒)\n",
    "def cacl_time_difference(start_time_str, end_time_str):\n",
    "    start_time = datetime.strptime(start_time_str,'%Y-%m-%d %H:%M:%S')\n",
    "    end_time = datetime.strptime(end_time_str,'%Y-%m-%d %H:%M:%S')\n",
    "    return int((end_time - start_time).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #读取log，生成携带用户行为的复合型字典\n",
    "# def create_user_behavior_dict(increment_log_data, user_behavior_dict):\n",
    "    \n",
    "#     for x in increment_log_data:\n",
    "#         if x['sip'] != '':\n",
    "            \n",
    "#             if x['sip'] not in user_behavior_dict.keys():\n",
    "#                 user_behavior_dict[x['sip']] = list()\n",
    "\n",
    "#             if len(user_behavior_dict[x['sip']]) == 0: #opreation内没有op_detail,则创建op_detail\n",
    "#                 user_behavior_dict[x['sip']].append(create_op_detail(x))\n",
    "\n",
    "#             if len(user_behavior_dict[x['sip']]) > 0: #opreation内已存在op_detail，此时需要循环该list的所有dict，比较时间差，如果两者时间相差<120s，更新结束时间，添加url；否则继续遍历比较直至找不到合适的区间为止，若找不到合适区间，新建\n",
    "#                 new_opreation = 1\n",
    "#                 length = len(user_behavior_dict[x['sip']])\n",
    "#                 i = 0\n",
    "#                 for op_detail in user_behavior_dict[x['sip']]:\n",
    "#                     time_difference = cacl_time_difference(op_detail['end_time'], x['@timestamp'])\n",
    "#                     if time_difference < 120:\n",
    "#                         op_detail['end_time'] = x['@timestamp']\n",
    "#                         op_detail['urls'].append(deal_url(x['request']))\n",
    "#                         new_opreation = 0\n",
    "#                         break\n",
    "#                     if new_opreation == 1 and i+1 == length:\n",
    "#                         user_behavior_dict[x['sip']].append(create_op_detail(x))\n",
    "#                         break\n",
    "                        \n",
    "#                     i += 1\n",
    "                    \n",
    "                \n",
    "#     return user_behavior_dict\n",
    "\n",
    "\n",
    "#读取log，生成携带用户行为的复合型字典\n",
    "def create_user_behavior_dict(increment_log_data, user_behavior_dict):\n",
    "    \n",
    "    for x in increment_log_data:\n",
    "        if x['sip'] != '':\n",
    "            \n",
    "            if x['sip'] not in user_behavior_dict.keys():\n",
    "                user_behavior_dict[x['sip']] = list()\n",
    "\n",
    "            if len(user_behavior_dict[x['sip']]) == 0: #opreation内没有op_detail,则创建op_detail\n",
    "                user_behavior_dict[x['sip']].append(create_op_detail(x))\n",
    "            else: #opreation内已存在op_detail，此时需要循环该list的所有dict，找到未填充满num_urls_in_op的记录\n",
    "                new_opreation = 1\n",
    "                i = 0\n",
    "                for op_detail in user_behavior_dict[x['sip']]:\n",
    "                    if len(op_detail['urls']) < num_urls_in_op:\n",
    "                        op_detail['end_time'] = x['@timestamp']\n",
    "                        op_detail['urls'].append(deal_url(x['request']))\n",
    "                        new_opreation = 0\n",
    "                        break\n",
    "                if new_opreation == 1:\n",
    "                    user_behavior_dict[x['sip']].append(create_op_detail(x))\n",
    "                    i += 1\n",
    "                    \n",
    "                \n",
    "    return user_behavior_dict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_behavior_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "user_behavior_dict = create_user_behavior_dict(increment_log_data, user_behavior_dict)\n",
    "end_time = datetime.now()\n",
    "print((end_time - start_time).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义模板list\n",
    "templates = list()\n",
    "templates.append(['GET /common/.../notice.do','POST /common/.../chooseHospital.do','GET /common/.../chooseExpert.do','GET /common/.../deptList.do','POST /.../getDept.do','GET /common/.../scheduleByDept.do','POST /.../scheduleByDept.do','GET /common/.../arrangementList.do','POST /.../getTimeList.do','GET /common/.../reserveConfirm.do','POST /.../reserveConfirm.do','POST /.../reserve.do']) #普通专家预约\n",
    "templates.append(['GET /common/.../notice.do','POST /common/.../chooseHospital.do','GET /common/.../chooseExpert.do','GET /common/.../expertNotice.do','POST /common/.../deptList.do','POST /.../getDept.do','GET /common/.../scheduleByDept.do','POST /.../scheduleByDept.do','GET /common/.../arrangementList.do','POST /.../getTimeList.do','GET /common/.../reserveConfirm.do','POST /.../reserveConfirm.do','POST /.../reserve.do'])#知名专家预约\n",
    "templates.append(['GET /common/.../notice.do','POST /common/.../chooseHospital.do','GET /common/.../chooseExpert.do','GET /common/.../specialClinic.do','POST /.../getDepts.do','GET /common/.../specialSchedule.do','POST /.../specialSchedule.do','GET /common/.../arrangementList.do','POST /.../getTimeList.do','GET /common/.../reserveConfirm.do','POST /.../reserveConfirm.do','POST /.../reserve.do'])#特色门诊预约\n",
    "templates.append(['GET /common/.../patientinfo.do','GET /common/.../reserveList.do','POST /.../getReserveList.do','GET /common/.../reserveDetail.do','POST /.../cancelReserve.do'])#取消预约\n",
    "templates.append(['GET /common/.../patientinfo.do','GET /common/.../patientMgr.do ','GET /.../patientinfo.do','GET /common/.../addPatient.do','POST /.../checkIdCard.do','POST /.../create.do'])#新增就诊人\n",
    "templates.append(['GET /common/.../patientinfo.do','GET /common/.../patientMgr.do ','GET /common/.../editPatient.do','POST /.../getPatientInfoById.do','POST /.../setDefault.do',''])#设置默认就诊人\n",
    "templates.append(['GET /common/.../patientinfo.do','GET /common/.../patientMgr.do ','GET /common/.../editPatient.do','POST /.../getPatientInfoById.do','GET /common/.../bind.do','POST /.../bindHosCard.do'])#绑卡\n",
    "templates.append(['GET /common/.../patientinfo.do','GET /common/.../patientMgr.do ','GET /common/.../editPatient.do','POST /.../getPatientInfoById.do','GET /.../deletePatient.do'])#删除就诊人\n",
    "templates.append(['GET /.../init.do','GET /common/.../reportType.do ','GET /common/.../reportList.do','GET /.../getReportList.do','GET /common/.../reportDetail.do','GET /.../getReportDeatil.do'])#门诊报告\n",
    "templates.append(['GET /.../init.do','GET /common/.../reportType.do ','GET /common/.../reportList.do','GET /.../getReportInList.do','GET /.../getReportDeatil.do'])#住院报告\n",
    "templates.append(['GET /.../init.do','GET /common/.../index.do','GET /common/.../balance.do','GET /.../patientBalance.do'])#门诊预交金\n",
    "templates.append(['GET /.../init.do','GET /common/.../index.do','GET /common/.../hosBalance.do','POST /.../inBalance.do'])#住院预交金\n",
    "templates.append(['GET /.../init.do','GET /common/.../index.do','GET /common/.../outpFeeList.do','POST /.../feeList.do','GET /common/.../feeDetail.do','POST /.../feeDetail.do'])#门诊费用\n",
    "templates.append(['GET /.../init.do','GET /common/.../index.do','GET /common/.../inpFeeList.do','POST /.../feeList.do','GET /common/.../feeDetail.do','POST /.../feeDetail.do'])#住院费用\n",
    "templates.append(['GET /.../init.do','GET /common/.../reserveQueueList.do','POST /.../getQueueList.do'])#候诊队列\n",
    "templates.append(['POST coreServlet'])#处理微信发送来的数据\n",
    "templates.append(['GET common/.../arrangementList.do', 'GET /.../404.do','POST /.../getTimeList.do'])#异常模板1\n",
    "#templates.append(['POST /.../getTimeList.do','POST /.../getTimeList.do','GET common/.../arrangementList.do','GET /.../404.do','GET common/.../arrangementList.do','GET /.../404.do'])#异常模板1\n",
    "templates.append(['GET common/.../scheduleByDept.do','POST /.../scheduleByDept.do','POST /.../getTimeList.do','GET common/.../arrangementList.do','POST /.../getTimeList.do', 'POST /.../getTimeList.do','POST /.../getTimeList.do','POST /.../getTimeList.do', 'POST /.../getTimeList.do'])#异常模板2\n",
    "#templates.append(['GET common/.../arrangementList.do', 'GET common/.../arrangementList.do', 'GET common/.../arrangementList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do', 'POST /.../getTimeList.do'])#异常模板3\n",
    "templates.append(['GET /common/.../deptList.do','POST /.../getDept.do','GET /common/.../scheduleByDept.do','POST /.../scheduleByDept.do','GET /common/.../arrangementList.do','POST /.../getTimeList.do'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probability_dict = dict()\n",
    "\n",
    "for x in user_behavior_dict.keys():\n",
    "    for op_detail in user_behavior_dict[x]:\n",
    "        matched_operations = np.zeros(len(templates),dtype='float32')\n",
    "        for idx,template in enumerate(templates):\n",
    "            n_template = len(template)\n",
    "            count = 0\n",
    "            probability_count = 0\n",
    "            for str in template:\n",
    "                if str in op_detail['urls']:\n",
    "                    probability_count += 1\n",
    "            for url in op_detail['urls']:\n",
    "                if url in template:\n",
    "                    count += 1\n",
    "            n_op = max(len(op_detail['urls']),16)\n",
    "            matched_operations[idx] = (probability_count / n_template) * (count / n_op)\n",
    "        if x not in probability_dict.keys():\n",
    "            probability_dict[x] = matched_operations.reshape([1,19])\n",
    "        else:\n",
    "            probability_dict[x] = np.vstack((probability_dict[x],matched_operations))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18662"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "for x in user_behavior_dict.keys():\n",
    "    idx += len(user_behavior_dict[x])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_behavior_dict['223.96.152.23'][2]['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#速度变慢的罪魁祸首\n",
    "def judge_time_range(start_time_str, end_time_str, time_str):\n",
    "    start_date = time.strptime(start_time_str,'%Y-%m-%d %H:%M:%S')\n",
    "    end_date = time.strptime(end_time_str,'%Y-%m-%d %H:%M:%S')\n",
    "    time_date = time.strptime(time_str,'%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if time_date > start_date and time_date < end_date:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!!!生成出现过的ip列表unique_dict\n",
    "#为每一个ip计算他访问过多少唯一页面，占访问次数的比例。frequency这英文似乎给错了，不是频率，而是覆盖率coverage \n",
    "\n",
    "\n",
    "unique_dict = dict()\n",
    "for x in increment_log_data:\n",
    "    ip = x['sip']\n",
    "    if ip not in unique_dict.keys():\n",
    "        unique_dict[ip] = dict()\n",
    "        unique_dict[ip]['coverage'] = 0\n",
    "        unique_dict[ip]['urls'] = list()\n",
    "    url = deal_url(x['request'])\n",
    "    unique_dict[ip]['urls'].append(url)\n",
    "    \n",
    "for ip in unique_dict.keys():\n",
    "    unique_list = list()\n",
    "    for url in unique_dict[ip]['urls']:\n",
    "        if url not in unique_list:\n",
    "            unique_list.append(url)\n",
    "    #print(len(unique_dict[ip]['urls']))\n",
    "    unique_dict[ip]['coverage'] = len(unique_list) / len(unique_dict[ip]['urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_operation_label(ip, time):\n",
    "#     global probability_dict\n",
    "#     class_type = 0\n",
    "#     for x in user_behavior_dict:\n",
    "#         if x == ip:\n",
    "#             index = 0\n",
    "#             for op_detail in user_behavior_dict[x]:\n",
    "#                 start_str = op_detail['start_time']\n",
    "#                 end_str = op_detail['end_time']\n",
    "#                 if judge_time_range(start_str, end_str, time):\n",
    "#                     class_type = probability_dict[ip][:,16:].sum()\n",
    "#                     #if probability_dict[ip][:,16:].sum() > 1:\n",
    "#                         #class_type = 1\n",
    "#                     #elif probability_dict[ip][index,17] > 0.4:\n",
    "#                         #class_type = 1\n",
    "#                    # elif probability_dict[ip][index,18] > 0.4:\n",
    "#                         #class_type = 1\n",
    "#                     break #如果匹配到了，也给了class_type了，还继续循环干什么\n",
    "#                 else:\n",
    "#                     index += 1\n",
    "#     return class_type\n",
    "\n",
    "\n",
    "# def get_operation_label(ip):\n",
    "#     global unique_dict\n",
    "#     class_type = 0\n",
    "#     if unique_dict[ip]['coverage'] < 0.1:\n",
    "#         class_type = 1\n",
    "#     return class_type\n",
    "\n",
    "import json\n",
    "with open('label_json.txt', 'r') as f:\n",
    "    labels_dict = json.loads(f.read())\n",
    "def get_operation_label(ip):\n",
    "    global labels_dict\n",
    "    return labels_dict[ip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018847457627118643"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exception_ip_list = list()\n",
    "for x in unique_dict.keys():\n",
    "    if unique_dict[x]['coverage'] < 0.15:\n",
    "        exception_ip_list.append(x)\n",
    "len(exception_ip_list)/len(unique_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_operation_label('27.204.15.164')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------start\n",
    "class IPAccessInfo(object):\n",
    "    def __init__(self, all_url_list, all_url_dict_list):\n",
    "        self.all_url_list = all_url_list\n",
    "        self.all_url_dict_list = all_url_dict_list\n",
    "        \n",
    "import pickle\n",
    "fp = open('url_info.pkl','rb',True)\n",
    "iPAccessInfo = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding\n",
    "\n",
    "# #列出唯一的url\n",
    "# url_dict = dict()\n",
    "# for urls in iPAccessInfo.all_url_list:\n",
    "#     for url in urls:\n",
    "#         if url not in url_dict.keys():\n",
    "#             url_dict[url] = 1\n",
    "#         else:\n",
    "#             url_dict[url] = url_dict[url] + 1\n",
    "            \n",
    "# url_list = list(url_dict.keys())\n",
    "\n",
    "# #将url embedding，转为固定长度的向量\n",
    "# num_urls = len(url_list)\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(num_urls, 32, input_length=1))\n",
    "# model.compile('rmsprop', 'mse')\n",
    "\n",
    "# input_array = np.array(range(num_urls))   #生成一个[num_url,1]的矩阵\n",
    "# output_array = model.predict(input_array)\n",
    "# print (output_array.shape)\n",
    "# print (output_array)\n",
    "\n",
    "# # 建立url_dict备查\n",
    "# url_dict = dict()\n",
    "# idx = 0\n",
    "# for url in url_list:\n",
    "#     url_dict[url] = output_array[idx,0,:]\n",
    "#     idx = idx + 1\n",
    "\n",
    "\n",
    "import pickle\n",
    "fp = open('url_embedding.pkl','rb',True)\n",
    "url_dict = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#老版本数据转换，将按照时间窗口、ip进行归类的urls列表，转化为时序data\n",
    "# def operations_to_sequence(all_url_dict_list):\n",
    "#     operation_matrix = np.zeros((len(all_url_dict_list),20,32))\n",
    "#     ip_operation_index = 0\n",
    "#     for ip_operations in all_url_dict_list: #对于每小段窗口，以ip归类的urls集合\n",
    "#         ip_operation_url_index = 0\n",
    "#         for url in ip_operations['urls']: #如果是dict字典的话\n",
    "#             url_vector = url_dict[url]\n",
    "#             operation_matrix[ip_operation_index, ip_operation_url_index, :] = url_vector\n",
    "#             ip_operation_url_index = ip_operation_url_index + 1\n",
    "#             if ip_operation_url_index>=20:  #不超过20个长度的序列，可以改\n",
    "#                 break\n",
    "#         ip_operation_index = ip_operation_index + 1\n",
    "#     return operation_matrix\n",
    "\n",
    "\n",
    "#新版本数据转换，将按照ip地址，然后里面的几段行为数据，转化为时序data\n",
    "# 数据格式为：\n",
    "# {'223.104.186.241': [{'end_time': '2017-07-13 06:25:32',\n",
    "#    'start_time': '2017-07-13 06:25:02',\n",
    "#    'urls': ['POST /.../getDept.do',\n",
    "#     'POST /.../getDept.do',\n",
    "#     'GET common/.../scheduleByDept.do',\n",
    "#     'POST /.../scheduleByDept.do',\n",
    "#     'GET common/.../arrangementList.do',\n",
    "#     'GET common/.../arrangementList.do',\n",
    "#     'POST /.../getTimeList.do',\n",
    "#     'GET common/.../reserveConfirm.do',\n",
    "#     'POST /.../reserveConfirm.do',\n",
    "#     'GET common/.../patientList.do',\n",
    "#     'GET /.../patientinfo.do',\n",
    "#     'GET common/.../reserveConfirm.do',\n",
    "#     'POST /.../reserveConfirm.do',\n",
    "#     'POST /.../reserve.do']},\n",
    "#   {'end_time': '2017-07-13 07:31:18',\n",
    "#    'start_time': '2017-07-13 07:26:36',\n",
    "#    'urls': ['GET /.../noticePage.html',\n",
    "\n",
    "def operations_to_sequence(user_behavior_dict):\n",
    "    #对于每一个ip，生成一组数据，数据shape为(n_ip, n_op, n_url, 32)\n",
    "    # 其中n_url就是timesteps，统一设置为20\n",
    "    # 其中n_op，目前统一设置为5\n",
    "    n_url = num_urls_in_op\n",
    "    n_op = n_ops_in_behavior\n",
    "\n",
    "    operation_matrix = np.zeros((len(user_behavior_dict), n_op, n_url, 32))\n",
    "    ip_operation_index = 0\n",
    "    ip_index = 0\n",
    "    ip_detail_url_index = 0\n",
    "    \n",
    "    for ip in user_behavior_dict.keys(): #对于每个ip,得到op_detail列表\n",
    "        ip_operation_index = 0\n",
    "        for op_detail in user_behavior_dict[ip]: #对于每一组urls操作\n",
    "            ip_detail_url_index = 0\n",
    "            for url in op_detail['urls']: #对于每一个url\n",
    "                url_vector = url_dict[url]  #翻译为url对应的vector\n",
    "                operation_matrix[ip_index, ip_operation_index, ip_detail_url_index, :] = url_vector\n",
    "                ip_detail_url_index += 1\n",
    "                if ip_detail_url_index>=n_url:  #不超过30个长度的序列，可以改\n",
    "                    break\n",
    "                    \n",
    "            ip_operation_index += 1\n",
    "            if ip_operation_index >= n_op: #不超过10个操作组：\n",
    "                break\n",
    "                \n",
    "        ip_index+=1\n",
    "    return operation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7374, 10, 20, 32)\n"
     ]
    }
   ],
   "source": [
    "data = operations_to_sequence(user_behavior_dict)\n",
    "print (data.shape) #数据编码，我就是快，数据量大640倍，还是快。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_class = 2\n",
    "# 旧版的数据转换\n",
    "# def label_operations(all_url_dict_list):\n",
    "#     operation_label = np.zeros(len(all_url_dict_list)) # 先按照0 1 2 3，一维数组排序\n",
    "#     ip_operation_index = 0\n",
    "#     for ip_operations in all_url_dict_list: #对于每小段窗口，以ip归类的urls集合\n",
    "#         operation_label[ip_operation_index] = get_operation_label(ip_operations['ip'],ip_operations['time'])\n",
    "#         ip_operation_index = ip_operation_index + 1\n",
    "        \n",
    "#     #转化为one-hot编码的label\n",
    "#     return operation_label\n",
    "\n",
    "\n",
    "def label_operations(user_behavior_dict):\n",
    "    #计算这个dict中有多少段操作\n",
    "        \n",
    "    operation_labels = np.zeros(len(user_behavior_dict)) # 先按照0 1 2 3，一维数组排序\n",
    "    ip_index = 0\n",
    "    for ip in user_behavior_dict.keys(): #对于每个ip,得到op_detail列表\n",
    "        operation_labels[ip_index] = get_operation_label(ip)\n",
    "        ip_index += 1\n",
    "            \n",
    "    return operation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7374,)\n"
     ]
    }
   ],
   "source": [
    "# tmd为啥这么慢，为啥这么慢，为啥这么慢啊!!!!!!\n",
    "labels = label_operations(user_behavior_dict)\n",
    "print (labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6201"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels==0).sum() #验证每个类别的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#网址embedding表示，应该不需要进行正规化吧\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# #由于时序序列数据，是（？，时间长度，数据长度）的，所以应该执行一个reshape\n",
    "# X = scaler.fit_transform(data.reshape([-1,20*32]))\n",
    "# X = X.reshape([-1,20,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义保存数据的类\n",
    "import numpy as np\n",
    "\n",
    "#定义类\n",
    "class IPAccessData(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "#         self.ip_dict = ip_dict\n",
    "#         self.ip_reserve_list = ip_reserve_list\n",
    "#         self.data_scaler = data_scaler\n",
    "\n",
    "# 保存文件为pkl\n",
    "import pickle\n",
    "\n",
    "iPAccessData = IPAccessData(data,labels)\n",
    "fp = open('iPAccessData_behavior.pkl','wb',True)\n",
    "pickle.dump(iPAccessData, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
